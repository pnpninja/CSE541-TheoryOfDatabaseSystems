 ____            __                                           
|  _ \ ___ _ __ / _| ___  _ __ _ __ ___   __ _ _ __   ___ ___ 
| |_) / _ \ '__| |_ / _ \| '__| '_ ` _ \ / _` | '_ \ / __/ _ \
|  __/  __/ |  |  _| (_) | |  | | | | | | (_| | | | | (_|  __/
|_|   \___|_|  |_|  \___/|_|  |_| |_| |_|\__,_|_| |_|\___\___|
                                                              


These are the performance statistics - 
 _____________________________________________________________________________________________________________________________________________
| Job Type   | Runtime for Total Number of Deaths per Location | Runtime for Total Number of Cases per 1 million population for every country |
|------------|-------------------------------------------------|------------------------------------------------------------------------------|
| Hadoop Job | 3.565 seconds                                   | 2.603 seconds                                                                |
| Spark Job  | 3.183 seconds                                   | 3.158 seconds                                                                |
 =============================================================================================================================================



How to measure the performance statistics - 

For Hadoop Jobs - we can track on the Hadoop Cluster Portal. This gives info on the job execution time.
However, additional time is taken for data communication from Hadoop to JVM. To measure total time in Java program, we use `Instant` class

To measure time taken in critical section, we do this - 

	Instant start = Instant.now();
		//
		//Critical section
		//
	Instant finish = Instant.now();
	long timeElapsed = Duration.between(start, finish).toMillis();
	System.out.println("Time taken - " + timeElapsed + "ms");

For Spark Jobs, we time the total execution time in code using the code block mentioned above.